{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### READ ME ###\n",
    "\n",
    "# Last Updated 4/14/2021\n",
    "\n",
    "# Best-Fit Mapping allows the user to generate attribute maps that limit interpolation to a user-defined error threshold\n",
    "# Code accepts .csv files and is hard-coded to accept certain column names (see below)\n",
    "# Code outputs .csv's 1) mapped point data 2) best fit grid 3) smoothed grid 4) mapped point data with sampled grid values\n",
    "# Code outputs .jpg's for maps and select plots\n",
    "# User must edit code blocks to import csv, and export to a specified folder\n",
    "\n",
    "# Hard coded column names (captialization required):\n",
    "#### Required column names: X, Y or LAT, LON\n",
    "#### Nonrequired utilized column names: UWI, ISO (Isopach)\n",
    "#### Curve footages > 0 with '_GT0' in column name will be used with ISO to calculate curve coverage (not required)\n",
    "#### Curve attributes will be binned appropriately when named starting with: GR, RESD, PHIN, PHID, DTC, DRHO (not required)\n",
    "# Hard coded shapefiles for counties and faults\n",
    "# Toggle for .jpg popups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from geopandas import *\n",
    "from shapely.geometry import Polygon\n",
    "import numpy as np\n",
    "np.warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from scipy.spatial import cKDTree, KDTree\n",
    "from statistics import mean\n",
    "import re\n",
    "from pathlib import Path, PureWindowsPath\n",
    "import webbrowser\n",
    "from pyproj import Transformer, Proj, CRS\n",
    "\n",
    "class tree(object):\n",
    "    def __init__(self, X=None, z=None, leafsize=10):\n",
    "        if not X is None:\n",
    "            self.tree = cKDTree(X, leafsize=leafsize )\n",
    "        if not z is None:\n",
    "            self.z = np.array(z)\n",
    "\n",
    "    def fit(self, X=None, z=None, leafsize=10):\n",
    "        return self.__init__(X, z, leafsize)\n",
    "\n",
    "    def __call__(self, X, k=6, eps=1e-6, p=2, regularize_by=1e-9):\n",
    "        self.distances, self.idx = self.tree.query(X, k, eps=eps, p=p)\n",
    "        self.distances += regularize_by\n",
    "        weights = self.z[self.idx.ravel()].reshape(self.idx.shape)\n",
    "        mw = np.sum(weights/self.distances, axis=1) / np.sum(1./self.distances, axis=1)\n",
    "        return mw\n",
    "\n",
    "    def transform(self, X, k=6, p=2, eps=1e-6, regularize_by=1e-9):\n",
    "        return self.__call__(X, k, eps, p, regularize_by)\n",
    "\n",
    "def merge(list1, list2):    \n",
    "    merged_list = [(list1[i], list2[i]) for i in range(0, len(list1))]\n",
    "    return np.array(merged_list)\n",
    "\n",
    "###############################\n",
    "### ***Linked Shapefiles*** ###\n",
    "###############################\n",
    "file_path = 'R:/Data Analytics/Competitor analysis/GIDDINGS_CHALK/GEOLOGY/MAPPING/'\n",
    "\n",
    "cnty_shapefile = gpd.read_file(\"R:/Data Analytics/Competitor analysis/GIDDINGS_CHALK/ARCMAP/SHAPEFILES/COUNTY_OUTLINES_4204_CROP2.shp\")\n",
    "flt_shapefile = gpd.read_file('R:/Data Analytics/Misc Requests/Sanchez_EGFD/PCV_WORK_FILES/Shapefiles/Mexia_Talco_Faults.shp')\n",
    "flt_shapefile2 = gpd.read_file(\"R:\\Data Analytics\\Competitor analysis\\GIDDINGS_CHALK\\ARCMAP\\SHAPEFILES\\ALL_CHALK_FAULTS_4204_CROP.shp\")\n",
    "prj_aoi = gpd.read_file(\"R:/Data Analytics/Competitor analysis/GIDDINGS_CHALK/GEOLOGY\\MAPPING/PROJECT_OUTLINE_4204.shp\")\n",
    "\n",
    "####################################\n",
    "### ***TOGGLES FOR JPG POPUPS*** ###\n",
    "####################################\n",
    "# 1 for popup, 0 for none\n",
    "outlier_popup = 0\n",
    "bf_popup = 1\n",
    "smth_popup = 1\n",
    "cfp_popup = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################################\n",
    "### ***LINKED DATA TABLES*** ###\n",
    "################################\n",
    "\n",
    "data = pd.read_csv(file_path + 'GID_ASTNU_ATTRIBUTES.csv')\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_lst = []\n",
    "\n",
    "for col in data.columns:\n",
    "    print(col)\n",
    "    col_lst.append(col)\n",
    "    \n",
    "if 'LAT' in col_lst and 'LON' in col_lst and 'X' in col_lst and 'Y' in col_lst:\n",
    "    print('Location data ready')\n",
    "    \n",
    "elif 'X' in col_lst and 'Y' in col_lst and 'LAT' not in col_lst and 'LON' not in col_lst:\n",
    "    print('Converting XY to Lat/Long...')\n",
    "    \n",
    "    #Convert xy to lat long\n",
    "    inproj = 32040\n",
    "    outproj = 4267\n",
    "    transformer = Transformer.from_crs(inproj, outproj)\n",
    "    xin, yin = data['X'].tolist(), data['Y'].tolist()\n",
    "    latout, lonout = transformer.transform(xin, yin)\n",
    "    data['LAT'] = latout\n",
    "    data['LON'] = lonout\n",
    "    \n",
    "elif 'LAT' in col_lst and 'LON' in col_lst and 'X' not in col_lst and 'Y' not in col_lst:\n",
    "    print('Converting Lat/Long to XY...')\n",
    "    \n",
    "    #Convert lat long to xy\n",
    "    inproj = 4267\n",
    "    outproj = 32040\n",
    "    print('check and check')\n",
    "    transformer = Transformer.from_crs(inproj, outproj, always_xy=True)\n",
    "    lonin = data['LON'].tolist()\n",
    "    latin = data['LAT'].tolist()\n",
    "    xout, yout = transformer.transform(lonin, latin)\n",
    "    data['X'] = xout\n",
    "    data['Y'] = yout\n",
    "    \n",
    "else:\n",
    "    print('Insufficient location data.')\n",
    "    \n",
    "    \n",
    "# COMMON EPSG CODES\n",
    "#   NAD27 LAT LONG          =   4267\n",
    "#   NAD27 TX CENTRAL US-FT  =   32039\n",
    "#   NAD27 TX S CENTRAL US-FT = 32040\n",
    "#   NAD27 LA NORTH US-FT    =   26781\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_lst = []\n",
    "new_col = 'blah'\n",
    "print('Columns to import: ')\n",
    "\n",
    "for col in data.columns:\n",
    "    print(col)\n",
    "    col_lst.append(col)\n",
    "\n",
    "man_lst = input('Use auto-generated list?: (Y- yes, N- start manual selection)')\n",
    "if man_lst == 'N':\n",
    "    col_lst = []\n",
    "    while True:\n",
    "        new_col = input('Enter name of attribue to import: (inclue X & Y, d for done)')\n",
    "        if new_col == 'd': break        \n",
    "        col_lst.append(new_col)\n",
    "    print('Manually entered columns to import:\\n', col_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorize Attributes by curve name & log transform RESD MN properties\n",
    "\n",
    "GR_crvs = ['UWI', 'X', 'Y', 'LAT', 'LON', 'ISO']\n",
    "RESD_crvs = ['UWI', 'X', 'Y', 'LAT', 'LON', 'ISO']\n",
    "PHIN_crvs = ['UWI', 'X', 'Y', 'LAT', 'LON', 'ISO']\n",
    "PHID_crvs = ['UWI', 'X', 'Y', 'LAT', 'LON', 'ISO']\n",
    "DTC_crvs = ['UWI', 'X', 'Y', 'LAT', 'LON', 'ISO']\n",
    "\n",
    "for col in col_lst:\n",
    "    crv_match = re.findall('^GR_*', col)\n",
    "    if len(crv_match) > 0:\n",
    "        GR_crvs.append(col)\n",
    "print('GR Curve Attributes: ', GR_crvs)\n",
    "\n",
    "for col in col_lst:\n",
    "    crv_match = re.findall('^RESD_*', col)\n",
    "    if len(crv_match) > 0:\n",
    "        RESD_crvs.append(col)\n",
    "    resd_mn_match = re.findall('RESD.+MN', str(col))\n",
    "    if len(resd_mn_match) > 0:\n",
    "        new_col = str(col) + '_LOG'\n",
    "        data[new_col] = np.log10(data[col])\n",
    "        data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        RESD_crvs.append(new_col)\n",
    "print('RESD Curve Attributes: ', RESD_crvs)\n",
    "\n",
    "for col in col_lst:\n",
    "    crv_match = re.findall('^PHIN_*', col)\n",
    "    if len(crv_match) > 0:\n",
    "        PHIN_crvs.append(col)\n",
    "print('PHIN Curve Attributes: ', PHIN_crvs)\n",
    "\n",
    "for col in col_lst:\n",
    "    crv_match = re.findall('^PHID_*', col)\n",
    "    if len(crv_match) > 0:\n",
    "        PHID_crvs.append(col)\n",
    "print('PHID Curve Attributes: ', PHID_crvs)\n",
    "\n",
    "for col in col_lst:\n",
    "    crv_match = re.findall('^DTC_*', col)\n",
    "    if len(crv_match) > 0:\n",
    "        DTC_crvs.append(col)\n",
    "print('DTC Curve Attributes: ', DTC_crvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate NTG and coverage for GT/LT attributes (gross properties and 'PAY_NT')\n",
    "\n",
    "crv_lst_lst = []\n",
    "crv_lst_lst.append(GR_crvs)\n",
    "crv_lst_lst.append(RESD_crvs)\n",
    "crv_lst_lst.append(PHIN_crvs)\n",
    "crv_lst_lst.append(PHID_crvs)\n",
    "crv_lst_lst.append(DTC_crvs)\n",
    "\n",
    "if 'ISO' in data:\n",
    "    for crv_lst_ in crv_lst_lst:\n",
    "        for prop in crv_lst_:\n",
    "            GT_match = re.findall('_[A-Z]T[0-9]+$', str(prop))\n",
    "            if len(GT_match) > 0:\n",
    "                gross_att_match = re.findall('_[A-Z]T0$', str(prop))\n",
    "                if len(gross_att_match) > 0:\n",
    "                    cov_nm = str(prop)\n",
    "                    cov_nm = cov_nm.split('_')\n",
    "                    cov_nm = cov_nm[0]\n",
    "                    cov_nm = str(cov_nm + '_COV')\n",
    "                    data[cov_nm] = data[prop]/data['ISO']\n",
    "                    data.loc[(data[cov_nm] > 1), cov_nm] = 1\n",
    "                    \n",
    "                    if prop.split('_')[0] == 'GR':\n",
    "                        GR_crvs.append(cov_nm)\n",
    "                    if prop.split('_')[0] == 'RESD':\n",
    "                        RESD_crvs.append(cov_nm)\n",
    "                    if prop.split('_')[0] == 'PHIN':\n",
    "                        PHIN_crvs.append(cov_nm)\n",
    "                    if prop.split('_')[0] == 'PHID':\n",
    "                        PHID_crvs.append(cov_nm)\n",
    "                    if prop.split('_')[0] == 'DTC':\n",
    "                        DTC_crvs.append(cov_nm)\n",
    "                else:\n",
    "                    NTG_var_nm = str(prop) + '_NTG'\n",
    "                    data[NTG_var_nm] = data[prop]/data['ISO']\n",
    "                    data.loc[(data[NTG_var_nm] > 1), NTG_var_nm] = 1\n",
    "                    \n",
    "                    if prop.split('_')[0] == 'GR':\n",
    "                        GR_crvs.append(NTG_var_nm)\n",
    "                    if prop.split('_')[0] == 'RESD':\n",
    "                        RESD_crvs.append(NTG_var_nm)\n",
    "                    if prop.split('_')[0] == 'PHIN':\n",
    "                        PHIN_crvs.append(NTG_var_nm)\n",
    "                    if prop.split('_')[0] == 'PHID':\n",
    "                        PHID_crvs.append(NTG_var_nm)\n",
    "                    if prop.split('_')[0] == 'DTC':\n",
    "                        DTC_crvs.append(NTG_var_nm)\n",
    "\n",
    "else:\n",
    "    \n",
    "    if len(GR_crvs) > 0:\n",
    "        data['GR_COV'] = 1\n",
    "        GR_crvs.append('GR_COV')\n",
    "    \n",
    "    if len(RESD_crvs) > 0:\n",
    "        data['RESD_COV'] = 1\n",
    "        RESD_crvs.append('RESD_COV')\n",
    "        \n",
    "    if len(PHIN_crvs) > 0:\n",
    "        data['PHIN_COV'] = 1\n",
    "        PHIN_crvs.append('PHIN_COV')\n",
    "        \n",
    "    if len(PHID_crvs) > 0:\n",
    "        data['PHID_COV'] = 1\n",
    "        PHID_crvs.append('PHID_COV')\n",
    "    \n",
    "    if len(DTC_crvs) > 0:\n",
    "        data['DTC_COV'] = 1\n",
    "        DTC_crvs.append('DTC_COV')   \n",
    "        \n",
    "if ('ISO' in data) & ('PAY' in data):\n",
    "    data['PAY_NTG'] = data['PAY']/data['ISO']\n",
    "    data.loc[(data['PAY_NTG'] > 1), 'PAY_NTG'] = 1\n",
    "                    \n",
    "print(data.describe())\n",
    "\n",
    "print('GR Curve Attributes: ', GR_crvs)\n",
    "print('RESD Curve Attributes: ', RESD_crvs)\n",
    "print('PHIN Curve Attributes: ', PHIN_crvs)\n",
    "print('PHID Curve Attributes: ', PHID_crvs)\n",
    "print('DTC Curve Attributes: ', DTC_crvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view histograms of data\n",
    "data.hist(column=col_lst, bins=50, grid=False, figsize=(16,12), color='#607c8e', zorder=2, rwidth=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Convert incorrectly aliased curves and populate missing PHID_MN data with calculation from RHOB_MN and viceversa\n",
    "if set(['PHID_MN', 'RHOB_MN']).issubset(data.columns):\n",
    "    data.loc[data['PHID_MN'] > 2.0, 'PHID_MN'] = (data['PHID_MN']-2.71)/(1-2.71)\n",
    "    data.loc[data['RHOB_MN'] < 2.0, 'RHOB_MN'] = (1-data['RHOB_MN'])*2.65*data['RHOB_MN']\n",
    "\n",
    "    data['PHID_MN'] = data['PHID_MN'].fillna((data['RHOB_MN']-2.71)/(1-2.71))\n",
    "    data['PHID_COV'] = data['PHID_COV'].fillna((data['RHOB_COV']))\n",
    "    data['RHOB_MN'] = data['RHOB_MN'].fillna(data['PHID_MN']*1+(1-data['PHID_MN'])*2.65)\n",
    "    data['RHOB_COV'] = data['RHOB_COV'].fillna((data['PHID_COV']))\n",
    "\n",
    "    data.loc[data['RHOB_MN'] < 2.0,'RHOB_MN'] = (1-data['RHOB_MN'])*2.65*data['RHOB_MN']\n",
    "    \n",
    "    #view updated histograms of data\n",
    "    data.hist(column=col_lst, bins=50, grid=False, figsize=(16,12), color='#607c8e', zorder=2, rwidth=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_LOGCOV_PCT = 0.8\n",
    "\n",
    "all_atts={}\n",
    "map_crv_lst = []\n",
    "map_crv_att_lst = []\n",
    "other_lst = ['UWI', 'X', 'Y']\n",
    "other_lst_len = 3\n",
    "if 'ISO' in data:\n",
    "    other_lst.append('ISO')\n",
    "    other_lst_len = 4\n",
    "\n",
    "if len(GR_crvs) > 6:\n",
    "    GR_data = data[data.columns.intersection(GR_crvs)]\n",
    "    GR_data = GR_data[(GR_data['GR_COV'] != None) & (GR_data['GR_COV']>= user_LOGCOV_PCT)]\n",
    "    all_atts['GR'] = 'GR'\n",
    "    all_atts['GR'] = [i for i in GR_data.columns]\n",
    "    print(GR_data)\n",
    "    map_crv_lst.append('GR')\n",
    "    for a in GR_crvs:\n",
    "        map_crv_att_lst.append(a)\n",
    "    \n",
    "if len(RESD_crvs) > 6:\n",
    "    RESD_data = data[data.columns.intersection(RESD_crvs)]\n",
    "    RESD_data = RESD_data[RESD_data['RESD_COV']>= user_LOGCOV_PCT]\n",
    "    all_atts['RESD'] = 'RESD'\n",
    "    all_atts['RESD'] = [i for i in RESD_data.columns]\n",
    "    map_crv_lst.append('RESD')\n",
    "    for a in RESD_crvs:\n",
    "        map_crv_att_lst.append(a)\n",
    "        \n",
    "if len(PHIN_crvs) > 6:\n",
    "    PHIN_data = data[data.columns.intersection(PHIN_crvs)]\n",
    "    PHIN_data = PHIN_data[PHIN_data['PHIN_COV']>= user_LOGCOV_PCT]\n",
    "    all_atts['PHIN'] = 'PHIN'\n",
    "    all_atts['PHIN'] = [i for i in PHIN_data.columns]\n",
    "    map_crv_lst.append('PHIN')\n",
    "    for a in PHIN_crvs:\n",
    "        map_crv_att_lst.append(a)\n",
    "        \n",
    "if len(PHID_crvs) > 6:\n",
    "    PHID_data = data[data.columns.intersection(PHID_crvs)]\n",
    "    PHID_data = PHID_data[PHID_data['PHID_COV']>= user_LOGCOV_PCT]\n",
    "    all_atts['PHID'] = 'PHID'\n",
    "    all_atts['PHID'] = [i for i in PHID_data.columns]\n",
    "    map_crv_lst.append('PHID')\n",
    "    for a in PHID_crvs:\n",
    "        map_crv_att_lst.append(a)\n",
    "        \n",
    "if len(DTC_crvs) > 6:\n",
    "    DTC_data = data[data.columns.intersection(DTC_crvs)]\n",
    "    DTC_data = DTC_data[DTC_data['DTC_COV']>= user_LOGCOV_PCT]\n",
    "    all_atts['DTC'] = 'DTC'\n",
    "    all_atts['DTC'] = [i for i in DTC_data.columns]\n",
    "    map_crv_lst.append('DTC')\n",
    "    for a in DTC_crvs:\n",
    "        map_crv_att_lst.append(a)\n",
    "        \n",
    "col_lst = []\n",
    "for col in data.columns:\n",
    "    col_lst.append(col)\n",
    "        \n",
    "for el in col_lst:\n",
    "    if el not in map_crv_att_lst:\n",
    "        other_lst.append(el)\n",
    "\n",
    "if len(other_lst) > other_lst_len:\n",
    "    other_data = data[data.columns.intersection(other_lst)]\n",
    "    all_atts['OTHER'] = 'OTHER'\n",
    "    all_atts['OTHER'] = [i for i in other_data.columns]\n",
    "    map_crv_lst.append('OTHER')\n",
    "\n",
    "print('Curves with attributes available to map: ')\n",
    "for c in map_crv_lst:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### ***ITERATE HERE*** ###\n",
    "##########################\n",
    "\n",
    "#Choose raw log to analyze\n",
    "log_choice = input('Enter desired available curve to view attributes: ')\n",
    "\n",
    "if log_choice == 'GR': \n",
    "    df_choice = GR_data\n",
    "    print(df_choice.describe())\n",
    "if log_choice == 'RESD':\n",
    "    df_choice = RESD_data\n",
    "    print(df_choice.describe())\n",
    "if log_choice == 'PHID': \n",
    "    df_choice = PHID_data\n",
    "    print(df_choice.describe())\n",
    "if log_choice == 'PHIN': \n",
    "    df_choice = PHIN_data\n",
    "    print(df_choice.describe())\n",
    "if log_choice == 'DTC': \n",
    "    df_choice = DTC_data\n",
    "    print(df_choice.describe())\n",
    "if log_choice == 'OTHER':\n",
    "    df_choice = other_data\n",
    "    print(df_choice.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose attribute to analyze\n",
    "geoatt = input('Enter desired geologic attribute: ')\n",
    "df_choice.hist(column=[geoatt], bins=50, grid=False, figsize=(12,8), color='#607c8e', zorder=2, rwidth=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "add_filter_min = input('Enter additional filter min: (Press Return for none) ')\n",
    "if len(add_filter_min) < 1: add_filter_min = df_choice[geoatt].min()\n",
    "add_filter_max = input('Enter additional filter max: (Press Return for none) ')\n",
    "if len(add_filter_max) < 1: add_filter_max = df_choice[geoatt].max()\n",
    "\n",
    "df_choice = df_choice[(df_choice[geoatt]>=float(add_filter_min)) & (df_choice[geoatt]<=float(add_filter_max))]\n",
    "#print(df_choice.head(20))\n",
    "df_choice.hist(column=[geoatt], bins=50, grid=False, figsize=(12,8), color='#607c8e', zorder=2, rwidth=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Calculate average distance between data points\n",
    "\n",
    "if 'UWI' in col_lst:\n",
    "    map_data = df_choice[['UWI', geoatt, 'X', 'Y']]\n",
    "else:\n",
    "    map_data = df_choice[[geoatt, 'X', 'Y']]\n",
    "map_data = map_data.dropna()\n",
    "print('Number of data points: ', len(map_data))\n",
    "\n",
    "data_dist = map_data[['X', 'Y']]\n",
    "data_dist_array = data_dist.to_numpy()\n",
    "\n",
    "dist_tree = KDTree(data_dist_array)\n",
    "dist, ind = dist_tree.query(data_dist_array, k=2)\n",
    "\n",
    "map_data['NN_DIST'] = dist[:,1]\n",
    "map_data.hist(column=['NN_DIST'], bins=50, grid=False, figsize=(12,8), color='#607c8e', zorder=2, rwidth=0.9)\n",
    "print(round(map_data['NN_DIST'].min(), 1), '- minimum distance between nearest neighbors')\n",
    "print(round(map_data['NN_DIST'].mean(), 1), '- mean distance between nearest neighbors')\n",
    "print(round(map_data['NN_DIST'].max(), 1), '- maximum distance between nearest neighbors')\n",
    "print(round(map_data['NN_DIST'].mean()/4, 1), '- prescribed grid node spacing')\n",
    "pres_x_cols = (map_data['X'].max()-map_data['X'].min())/(map_data['NN_DIST'].mean()/4)\n",
    "pres_y_rows = (map_data['Y'].max()-map_data['Y'].min())/(map_data['NN_DIST'].mean()/4)\n",
    "pres_ncol = (pres_x_cols+pres_y_rows)/2\n",
    "print(int(pres_ncol), '- prescribed number of rows and columns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#perform trend surface outlier analysis\n",
    "fm_nm = input('Enter formation name: ')\n",
    "comm = input('Enter comment addition for file name: (Underscore before, Return for none) ')\n",
    "order_inp = input('Enter desired trend order: ')\n",
    "order_inp = int(order_inp)\n",
    "user_ol_thresh = input('Enter desired percentile for outlier threshold: (0-1, 0.9, 0.95, etc)')\n",
    "user_ol_thresh = float(user_ol_thresh)\n",
    "\n",
    "ncol = input('Enter desired number of rows and columns: (Return uses prescribed values) ')\n",
    "if len(ncol)< 1 :\n",
    "    ncol = int(pres_ncol)\n",
    "    nx, ny = int(ncol), int(ncol)\n",
    "else:\n",
    "    nx, ny = int(ncol), int(ncol)\n",
    "\n",
    "def polyfit2d(x, y, z, order=order_inp):\n",
    "    ncols = (order + 1)**2\n",
    "    G = np.zeros((x.size, ncols))\n",
    "    ij = itertools.product(range(order+1), range(order+1))\n",
    "    for k, (i,j) in enumerate(ij):\n",
    "        G[:,k] = x**i * y**j\n",
    "    m, _, _, _ = np.linalg.lstsq(G, z)\n",
    "    return m\n",
    "\n",
    "def polyval2d(x, y, m):\n",
    "    order = int(np.sqrt(len(m))) - 1\n",
    "    ij = itertools.product(range(order+1), range(order+1))\n",
    "    z = np.zeros_like(x)\n",
    "    for a, (i,j) in zip(m, ij):\n",
    "        z += a * x**i * y**j\n",
    "    return z\n",
    "\n",
    "# Fit tight IDW grid of data\n",
    "data_z = map_data[[geoatt]].to_numpy()\n",
    "data_x = map_data[['X']].to_numpy()\n",
    "data_y = map_data[['Y']].to_numpy()\n",
    "if 'UWI' in col_lst:\n",
    "    data_uwi = map_data[['UWI']].to_numpy()\n",
    "red_data_z = data_z.reshape(len(data_z),)\n",
    "red_data_x = data_x.reshape(len(data_z),)\n",
    "red_data_y = data_y.reshape(len(data_z),)\n",
    "if 'UWI' in col_lst:\n",
    "    red_data_uwi = data_uwi.reshape(len(data_uwi),)\n",
    "\n",
    "zxy = merge(red_data_x, red_data_y)\n",
    "zval = red_data_z\n",
    "idw_tree = tree(zxy, zval)\n",
    "\n",
    "x_spacing = np.linspace(red_data_x.min()-10000, red_data_x.max()+10000, int(ncol))\n",
    "y_spacing = np.linspace(red_data_y.min()-10000, red_data_y.max()+10000, int(ncol))\n",
    "xy_grid = np.meshgrid(x_spacing, y_spacing)\n",
    "grid_shape = xy_grid[0].shape\n",
    "xy_grid = np.reshape(xy_grid, (2, -1)).T\n",
    "\n",
    "z_idw = idw_tree(xy_grid)\n",
    "\n",
    "# Fit an nth order, 2d polynomial\n",
    "m = polyfit2d(xy_grid[:,0],xy_grid[:,1],z_idw)\n",
    "\n",
    "# Evaluate it on a grid...\n",
    "xx, yy = np.meshgrid(np.linspace(int(xy_grid[:,0].min()), int(xy_grid[:,0].max()), nx), \n",
    "                         np.linspace(int(xy_grid[:,1].min()), int(xy_grid[:,1].max()), ny))\n",
    "zz = polyval2d(xx, yy, m)\n",
    "\n",
    "#turn grid data into input points\n",
    "zxy_rev = merge(xy_grid[:,0], xy_grid[:,1])\n",
    "zval_rev = zz.reshape(int(ncol)**2,)\n",
    "\n",
    "idw_tree_rev = tree(zxy_rev,zval_rev)\n",
    "\n",
    "#perform idw in reverse\n",
    "ts_smp = idw_tree_rev(zxy)\n",
    "\n",
    "newvar = geoatt + '_TS' + str(order_inp)\n",
    "\n",
    "if 'UWI' in col_lst:\n",
    "    ts_grid = {'UWI': red_data_uwi, 'Y': red_data_y, 'X': red_data_x, geoatt: red_data_z, newvar: ts_smp}\n",
    "else:\n",
    "    ts_grid = {'Y': red_data_y, 'X': red_data_x, geoatt: red_data_z, newvar: ts_smp}\n",
    "ts_grid_df = pd.DataFrame(data=ts_grid)\n",
    "ts_diff_var = newvar + '_ABS_DIFF'\n",
    "ts_grid_df[ts_diff_var] = abs(ts_grid_df[geoatt]-ts_grid_df[newvar])\n",
    "\n",
    "ol_thresh = ts_grid_df[ts_diff_var].quantile(user_ol_thresh)\n",
    "ol_remove = ts_grid_df[ts_grid_df[ts_diff_var]>ol_thresh]\n",
    "ol_remain = ts_grid_df[ts_grid_df[ts_diff_var]<=ol_thresh]\n",
    "    \n",
    "ts_map_title = newvar\n",
    "\n",
    "#generate new grid with outliers removed\n",
    "olr_data_z = ol_remain[[geoatt]].to_numpy()\n",
    "olr_data_x = ol_remain[['X']].to_numpy()\n",
    "olr_data_y = ol_remain[['Y']].to_numpy()\n",
    "olr_red_data_z = olr_data_z.reshape(len(olr_data_z),)\n",
    "olr_red_data_x = olr_data_x.reshape(len(olr_data_z),)\n",
    "olr_red_data_y = olr_data_y.reshape(len(olr_data_z),)\n",
    "\n",
    "olr_zxy = merge(olr_red_data_x, olr_red_data_y)\n",
    "olr_zval = olr_red_data_z\n",
    "olr_idw_tree = tree(olr_zxy, olr_zval)\n",
    "\n",
    "olr_z_idw = olr_idw_tree(xy_grid)\n",
    "\n",
    "\n",
    "# Plot\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, sharex=True, sharey=True, figsize=(25,25))\n",
    "plot_title = 'Trend Surface Outlier Analysis\\n\\n Trend order: '+ str(order_inp) + ' \\nOutlier percentile threshold: ' + str(user_ol_thresh)\n",
    "fig.suptitle(plot_title, fontsize=18)\n",
    "ax1.contourf(x_spacing, y_spacing, z_idw.reshape(grid_shape), cmap='CMRmap')\n",
    "#ax1.scatter(red_data_x,red_data_y,c='black')\n",
    "ax1.scatter(ol_remove.X, ol_remove.Y, c='lime', s=400, marker='x')\n",
    "ax1.title.set_text('Honored Z Data')\n",
    "ax3.contourf(x_spacing, y_spacing, zz.reshape(grid_shape), cmap='CMRmap')\n",
    "ax3.scatter(red_data_x,red_data_y,c='black')\n",
    "ax3.scatter(ol_remove.X, ol_remove.Y, c='lime', s=400, marker='x')\n",
    "ax3.title.set_text('Trend Surface Grid')\n",
    "ax4.contourf(x_spacing, y_spacing, zz.reshape(grid_shape), cmap='Greys')\n",
    "ax4.scatter(red_data_x,red_data_y,c=ts_grid_df[ts_diff_var], cmap='YlOrRd')\n",
    "ax4.scatter(ol_remove.X, ol_remove.Y, c='lime', s=400, marker='x')\n",
    "ax4.title.set_text('Data Point TS Agreement')\n",
    "ax2.title.set_text('Honored Z Data - Outliers Removed')\n",
    "ax2.contourf(x_spacing, y_spacing, olr_z_idw.reshape(grid_shape), cmap='CMRmap')\n",
    "ax2.scatter(red_data_x,red_data_y,c='black')\n",
    "\n",
    "cnty_shapefile.plot(ax=ax1, facecolor=\"none\", edgecolor='dimgrey')\n",
    "cnty_shapefile.plot(ax=ax2, facecolor=\"none\", edgecolor='dimgrey')\n",
    "cnty_shapefile.plot(ax=ax3, facecolor=\"none\", edgecolor='dimgrey')\n",
    "cnty_shapefile.plot(ax=ax4, facecolor=\"none\", edgecolor='dimgrey')\n",
    "flt_shapefile2.plot(ax=ax1, facecolor=\"none\", edgecolor='sienna')\n",
    "flt_shapefile2.plot(ax=ax2, facecolor=\"none\", edgecolor='sienna')\n",
    "flt_shapefile2.plot(ax=ax3, facecolor=\"none\", edgecolor='sienna')\n",
    "flt_shapefile2.plot(ax=ax4, facecolor=\"none\", edgecolor='sienna')\n",
    "prj_aoi.plot(ax=ax1, facecolor=\"none\", edgecolor='red', linewidth=2)\n",
    "prj_aoi.plot(ax=ax2, facecolor=\"none\", edgecolor='red', linewidth=2)\n",
    "prj_aoi.plot(ax=ax3, facecolor=\"none\", edgecolor='red', linewidth=2)\n",
    "prj_aoi.plot(ax=ax4, facecolor=\"none\", edgecolor='red', linewidth=2)\n",
    "\n",
    "\n",
    "plt.xlim(red_data_x.min()-10000, red_data_x.max()+10000)\n",
    "plt.ylim(red_data_y.min()-10000, red_data_y.max()+10000)\n",
    "\n",
    "plot_nm = file_path + fm_nm + '_' + geoatt + '_TS_OUTLIERS' + str(comm) +'.jpg'\n",
    "plot_nm1 = str(plot_nm)\n",
    "new_path = PureWindowsPath(plot_nm1)\n",
    "plt.savefig(plot_nm,bbox_inches='tight')    \n",
    "if outlier_popup == 1:\n",
    "    webbrowser.open(plot_nm)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('\\n', ts_grid_df.describe(percentiles= [.05, .1, .25, .5, .75,.9, .95]))\n",
    "\n",
    "map_data = ts_grid_df.copy()\n",
    "or_map_data = ts_grid_df[ts_grid_df[ts_diff_var]<=ol_thresh]\n",
    "or_map_data.hist(column=[geoatt], bins=50, grid=False, figsize=(12,8), color='#607c8e', zorder=2, rwidth=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#perform best-fit gridding of cleaned point data using IDW algorithm (https://github.com/paulbrodersen/inverse_distance_weighting/blob/master/idw.py)\n",
    "\n",
    "map_pts = or_map_data[['X', 'Y', geoatt]]\n",
    "map_pts = map_pts.dropna()\n",
    "pts_folderpathname = file_path + fm_nm + '_' + geoatt + '_PTS' + str(comm) +'.csv'\n",
    "map_pts.to_csv(pts_folderpathname, index=False)\n",
    "\n",
    "or_data_z = or_map_data[[geoatt]].to_numpy()\n",
    "or_data_x = or_map_data[['X']].to_numpy()\n",
    "or_data_y = or_map_data[['Y']].to_numpy()\n",
    "or_red_data_z = or_data_z.reshape(len(or_data_z),)\n",
    "or_red_data_x = or_data_x.reshape(len(or_data_z),)\n",
    "or_red_data_y = or_data_y.reshape(len(or_data_z),)\n",
    "data_z = map_data[[geoatt]].to_numpy()\n",
    "#print('data_z', data_z)\n",
    "data_x = map_data[['X']].to_numpy()\n",
    "data_y = map_data[['Y']].to_numpy()\n",
    "red_data_z = data_z.reshape(len(data_z),)\n",
    "red_data_x = data_x.reshape(len(data_z),)\n",
    "red_data_y = data_y.reshape(len(data_z),)\n",
    "\n",
    "xmin = map_data.X.min(); xmax = map_data.X.max() # range of x values\n",
    "ymin = map_data.Y.min(); ymax = map_data.Y.max() # range of y values\n",
    "\n",
    "rocol = input('Enter starting number of rows and columns: (return = 10)')\n",
    "if len(rocol) < 1: rocol = 10\n",
    "while True:\n",
    "    if int(rocol) <=0:\n",
    "        rocol = input('Enter a number greater than zero: ')\n",
    "        if len(rocol) < 1: rocol = '10'\n",
    "        continue\n",
    "    break\n",
    "\n",
    "error_thresh = input('Enter value of acceptable error threshold: ')\n",
    "error_thresh_pct = input('Enter absolute residual percentile cutoff: ')\n",
    "error_act = 1000000.0\n",
    "\n",
    "while error_act > float(error_thresh):\n",
    "    #generate map data xyz's\n",
    "    zxy = merge(or_red_data_x, or_red_data_y)\n",
    "    #print('zxy\\n', len(zxy))\n",
    "    #print(zxy)\n",
    "    zval = or_red_data_z\n",
    "    #print('zval\\n', len(zval))\n",
    "    #print(zval)\n",
    "    idw_tree = tree(zxy, zval)\n",
    "    zxy_with_outliers = merge(red_data_x, red_data_y)\n",
    "    #zval_with_outliers = red_data_z\n",
    "    #idw_tree_with_outliers = tree(zxy_with_outliers, zval_with_outliers)\n",
    "\n",
    "    #generate mesh grid over map data (with 10,000 ft overlap)\n",
    "    print('Trying rows and columns of: ', rocol)\n",
    "    x_spacing = np.linspace(red_data_x.min()-10000, red_data_x.max()+10000, int(rocol))\n",
    "    y_spacing = np.linspace(red_data_y.min()-10000, red_data_y.max()+10000, int(rocol))\n",
    "    xy_grid = np.meshgrid(x_spacing, y_spacing)\n",
    "    grid_shape = xy_grid[0].shape\n",
    "    xy_grid = np.reshape(xy_grid, (2, -1)).T\n",
    "    #print(xy_grid)\n",
    "    #print(xy_grid[:,1])\n",
    "\n",
    "    #generate idw grid\n",
    "    z_idw = idw_tree(xy_grid)\n",
    "    #print('z_idw', z_idw)\n",
    "\n",
    "    #turn grid data into input points\n",
    "    zxy_rev = merge(xy_grid[:,0], xy_grid[:,1])\n",
    "    zval_rev = z_idw\n",
    "    idw_tree_rev = tree(zxy_rev, zval_rev)\n",
    "\n",
    "    #perform idw in reverse\n",
    "    z_idw_rev = idw_tree_rev(zxy)\n",
    "    z_idw_rev_with_outliers = idw_tree_rev(zxy_with_outliers)\n",
    "\n",
    "    #create a datapoint evaluation table\n",
    "    smp_nm_or = geoatt + '_SMP_OR'\n",
    "    smp_nm = geoatt + '_SMP'\n",
    "    grid_eval = {'X': or_red_data_x, 'Y': or_red_data_y, geoatt: or_red_data_z, smp_nm_or: z_idw_rev}\n",
    "    grid_eval_df = pd.DataFrame(data = grid_eval)\n",
    "    if 'UWI' in col_lst:\n",
    "        grid_eval_all = {'UWI': red_data_uwi, 'X': red_data_x, 'Y': red_data_y, geoatt: red_data_z, smp_nm: z_idw_rev_with_outliers}\n",
    "    else:\n",
    "        grid_eval_all = {'X': red_data_x, 'Y': red_data_y, geoatt: red_data_z, smp_nm: z_idw_rev_with_outliers}\n",
    "    grid_eval_all_df = pd.DataFrame(data = grid_eval_all)\n",
    "\n",
    "    smp_diff_nm = smp_nm_or + '_ABSDIFF'\n",
    "    grid_eval_df[smp_diff_nm] = abs(grid_eval[geoatt]-grid_eval_df[smp_nm_or])\n",
    "    grid_eval_df['ERROR_THRESH'] = error_thresh\n",
    "    error_act = grid_eval_df[smp_diff_nm].quantile(float(error_thresh_pct))\n",
    "    print('Current error: ', error_act, '    Error goal: ', error_thresh)\n",
    "    rocol = int(rocol) + 10\n",
    "else:\n",
    "    rocol = rocol - 10\n",
    "    print('Final rows and columns chosen: ', rocol)\n",
    "    print('User-defined error threshold: ', error_thresh)\n",
    "    print('Final error actual: ', error_act)\n",
    "    \n",
    "    #Create grid df\n",
    "    bf_grid = {'X': xy_grid[:,0], 'Y': xy_grid[:,1], geoatt: z_idw}\n",
    "    bf_grid_df = pd.DataFrame(data=bf_grid)\n",
    "    bf_grid_df = bf_grid_df.dropna()\n",
    "    \n",
    "    #Convert df to geopandas geodataframe, spatial join with AOI to remove points outside\n",
    "    bf_grid_gdf = gpd.GeoDataFrame(bf_grid_df, geometry=gpd.points_from_xy(bf_grid_df.X, bf_grid_df.Y), \n",
    "                                   crs={'init' :'epsg:32040'})\n",
    "    bfgrid_gdf_clip = gpd.sjoin(bf_grid_gdf, prj_aoi, op = 'within')\n",
    "    \n",
    "    #Prep for figure    \n",
    "    fig, ax = plt.subplots(figsize=(25,25))\n",
    "\n",
    "    plt.contourf(x_spacing, y_spacing, z_idw.reshape(grid_shape), 100, cmap='CMRmap')\n",
    "    ax.scatter(map_data.X, map_data.Y, c='k', alpha=0.2, marker='.')\n",
    "    #ax.scatter(xy_grid[:,0], xy_grid[:,1], c='k', alpha =0.2, marker='x')\n",
    "    cbar = plt.colorbar(shrink=.5)\n",
    "    ci = plt.contour(x_spacing, y_spacing, z_idw.reshape(grid_shape), colors='black', linewidths=0.5)\n",
    "    ax.clabel(ci, inline=1, colors='black', fmt='%1.2f')\n",
    "\n",
    "    plot_title = fm_nm + '_' + geoatt + '_' + str(rocol) + 'RC_' + str(error_thresh) + '_THRESHOLD'\n",
    "    short_nm = fm_nm + '_' + geoatt + '_GRID'\n",
    "    plt.title(plot_title)\n",
    "    \n",
    "    #Create a data limts polygon\n",
    "    #data_lim_poly = Polygon([(red_data_x.min()-10000, red_data_y.min()-10000),\n",
    "    #                        (red_data_x.max()+10000, red_data_y.min()-10000),\n",
    "    #                        (red_data_x.max()+10000, red_data_y.max()+10000),\n",
    "    #                        (red_data_x.min()-10000, red_data_y.max()+10000)])\n",
    "    #data_lim_poly_gs = GeoSeries(data_lim_poly)\n",
    "    #data_lim_poly_gs.plot(ax=ax, facecolor='none', edgecolor='green')\n",
    "\n",
    "    cnty_shapefile['coords'] = cnty_shapefile['geometry'].apply(lambda x: x.representative_point().coords[:])\n",
    "    cnty_shapefile['coords'] = [coords[0] for coords in cnty_shapefile['coords']]\n",
    "    \n",
    "    for idx, row in cnty_shapefile.iterrows():\n",
    "        plt.annotate(s=row['COUNTY'], xy=row['coords'],\n",
    "                 horizontalalignment='center', c='dimgrey')\n",
    "    cnty_shapefile.plot(ax=ax, facecolor=\"none\", edgecolor='dimgrey', linewidth=2)\n",
    "    flt_shapefile2.plot(ax=ax, facecolor=\"none\", edgecolor='sienna')\n",
    "    prj_aoi.plot(ax=ax, facecolor=\"none\", edgecolor='red', linewidth=3)\n",
    "    plt.xlim(red_data_x.min()-10000, red_data_x.max()+10000)\n",
    "    plt.ylim(red_data_y.min()-10000, red_data_y.max()+10000)\n",
    "    \n",
    "    plot_text = 'Error threshold: ' + str(error_thresh) + '    Cutoff percentile: ' + str(error_thresh_pct)\n",
    "    ax.set_xlabel(plot_text, fontsize=12)\n",
    "    \n",
    "    plot_nm = file_path + fm_nm + '_' + geoatt + '_BF_GRID' + str(comm) +'.jpg'\n",
    "    plot_nm1 = str(plot_nm)\n",
    "    new_path = PureWindowsPath(plot_nm1)\n",
    "    plt.savefig(plot_nm,bbox_inches='tight')\n",
    "    if bf_popup == 1:\n",
    "        webbrowser.open(plot_nm)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    folderpathname = file_path + short_nm + str(comm)+'.csv'\n",
    "    bf_grid_df.to_csv(folderpathname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Smooth grid\n",
    "\n",
    "radius = input('Choose radius: (0- No smoothing, 2- 2 nodes, 3- 3 nodes)')\n",
    "\n",
    "if radius != '0':\n",
    "    smth_nm = geoatt + '_SMTH'\n",
    "    smth_output_df = pd.DataFrame(columns=['X', 'Y', smth_nm])\n",
    "\n",
    "    index_ref = 0\n",
    "    #print('original index ref', index_ref)\n",
    "\n",
    "    if radius == '2':\n",
    "        while True:\n",
    "            if index_ref > (len(bf_grid_df)-1): break\n",
    "            x_ref = bf_grid_df.loc[index_ref]['X']\n",
    "            #print('x_ref', x_ref)\n",
    "            y_ref = bf_grid_df.loc[index_ref]['Y']\n",
    "            #print('y_ref', y_ref)\n",
    "\n",
    "            ref_val = bf_grid_df.loc[index_ref][geoatt]\n",
    "            #print('ref_val', ref_val)\n",
    "            if index_ref != 0 : \n",
    "                ref_val_left = bf_grid_df.loc[index_ref-1][geoatt]\n",
    "            else: ref_val_left = ref_val\n",
    "            if index_ref != (len(bf_grid_df) - 1): \n",
    "                ref_val_right = bf_grid_df.loc[index_ref+1][geoatt]\n",
    "            else: ref_val_right = ref_val\n",
    "            if index_ref < int(rocol):\n",
    "                ref_val_up = ref_val\n",
    "            else: ref_val_up = bf_grid_df.loc[index_ref-int(rocol)][geoatt]\n",
    "            if index_ref >((int(rocol)*int(rocol)-int(rocol))-1):\n",
    "                ref_val_down = ref_val\n",
    "            else: ref_val_down = bf_grid_df.loc[index_ref+int(rocol)][geoatt]\n",
    "\n",
    "            if index_ref == 0:   #Top left corner\n",
    "                ref_lst = [ref_val, ref_val_right, ref_val_down]\n",
    "\n",
    "            if index_ref == (int(rocol)*int(rocol)):   #Bottom right corner\n",
    "                ref_lst = [ref_val, ref_val_left, ref_val_up]\n",
    "\n",
    "            elif ((index_ref+1)/int(rocol)).is_integer() == True:   #Right edge grid node\n",
    "                ref_lst = [ref_val, ref_val_left, ref_val_up, ref_val_down]\n",
    "\n",
    "            elif ((index_ref)/int(rocol)).is_integer() == True:   #Left edge grid node\n",
    "                ref_lst = [ref_val, ref_val_right, ref_val_up, ref_val_down]\n",
    "\n",
    "            elif ((index_ref+1)<int(rocol)):   #Top row of grid\n",
    "                ref_lst = [ref_val, ref_val_right, ref_val_left, ref_val_down]\n",
    "\n",
    "            elif ((index_ref+1)>(int(rocol)*int(rocol)-int(rocol))):   #Bottom row of grid\n",
    "                ref_lst = [ref_val, ref_val_right, ref_val_left, ref_val_up]\n",
    "\n",
    "            else:\n",
    "                ref_lst = [ref_val, ref_val_left, ref_val_right, ref_val_up, ref_val_down]\n",
    "\n",
    "            smooth_val = mean(ref_lst)\n",
    "\n",
    "            smth_output_df = smth_output_df.append({'X': x_ref, 'Y': y_ref, smth_nm: smooth_val}, ignore_index=True)\n",
    "\n",
    "            index_ref += 1\n",
    "            continue\n",
    "\n",
    "    if radius == '3':\n",
    "        avoid_lst = list()\n",
    "        start_num = 0\n",
    "        while True:\n",
    "            if start_num == int(rocol)*2: break\n",
    "            avoid_lst.append(start_num)\n",
    "            start_num += 1\n",
    "        start_num = int(rocol)*(int(rocol)-2)\n",
    "        while True:\n",
    "            if start_num == int(rocol)**2: break\n",
    "            avoid_lst.append(start_num)\n",
    "            start_num += 1\n",
    "        start_num = 0\n",
    "        while True:\n",
    "            if start_num == int(rocol)**2: break\n",
    "            avoid_lst.append(start_num)\n",
    "            start_num += int(rocol)\n",
    "        start_num = 1\n",
    "        while True:\n",
    "            if start_num >= int(rocol)**2: break\n",
    "            avoid_lst.append(start_num)\n",
    "            start_num += int(rocol)\n",
    "        start_num  = int(rocol)-2\n",
    "        while True:\n",
    "            if start_num >= int(rocol)**2: break\n",
    "            avoid_lst.append(start_num)\n",
    "            start_num += int(rocol)\n",
    "        start_num  = int(rocol)-1\n",
    "        while True:\n",
    "            if start_num >= int(rocol)**2: break\n",
    "            avoid_lst.append(start_num)\n",
    "            start_num += int(rocol)\n",
    "\n",
    "        while True:\n",
    "            if index_ref > (len(bf_grid_df)-1): break\n",
    "            x_ref = bf_grid_df.loc[index_ref]['X']\n",
    "            y_ref = bf_grid_df.loc[index_ref]['Y']\n",
    "\n",
    "            ref_val = bf_grid_df.loc[index_ref][geoatt]\n",
    "            if index_ref in avoid_lst:\n",
    "                smooth_val = ref_val\n",
    "            else:\n",
    "                ref_val_up = bf_grid_df.loc[index_ref-int(rocol)][geoatt]\n",
    "                ref_val_down = bf_grid_df.loc[index_ref+int(rocol)][geoatt]\n",
    "                ref_val_right = bf_grid_df.loc[index_ref+1][geoatt]\n",
    "                ref_val_left = bf_grid_df.loc[index_ref-1][geoatt]\n",
    "                ref_val_up2 = bf_grid_df.loc[index_ref-int(rocol)*2][geoatt]\n",
    "                ref_val_down2 = bf_grid_df.loc[index_ref+int(rocol)*2][geoatt]\n",
    "                ref_val_right2 = bf_grid_df.loc[index_ref+2][geoatt]\n",
    "                ref_val_left2 = bf_grid_df.loc[index_ref-2][geoatt]\n",
    "                ref_val_upright = bf_grid_df.loc[(index_ref-int(rocol))+1][geoatt]\n",
    "                ref_val_downright = bf_grid_df.loc[(index_ref+int(rocol))+1][geoatt]\n",
    "                ref_val_upleft = bf_grid_df.loc[(index_ref-int(rocol))-1][geoatt]\n",
    "                ref_val_downleft = bf_grid_df.loc[(index_ref+int(rocol))-1][geoatt]\n",
    "\n",
    "                ref_lst = [ref_val, ref_val_left, ref_val_right, ref_val_up, ref_val_down,\n",
    "                          ref_val_up2, ref_val_down2, ref_val_right2, ref_val_left2,\n",
    "                          ref_val_upright, ref_val_downright, ref_val_upleft, ref_val_downleft]\n",
    "\n",
    "                smooth_val = mean(ref_lst)\n",
    "\n",
    "            smth_output_df = smth_output_df.append({'X': x_ref, 'Y': y_ref, smth_nm: smooth_val}, ignore_index=True)\n",
    "\n",
    "            index_ref += 1\n",
    "            continue\n",
    "    print('smth_output_df\\n', smth_output_df)\n",
    "\n",
    "    smth_array = smth_output_df[smth_nm].values\n",
    "    fig, ax = plt.subplots(figsize=(15,15))\n",
    "\n",
    "    plt.contourf(x_spacing, y_spacing, smth_array.reshape(grid_shape), 100, cmap='CMRmap')\n",
    "    ax.scatter(map_data.X, map_data.Y, c='k', alpha=0.2, marker='.')\n",
    "    #ax.scatter(smth_output_df.X, smth_output_df.Y, c='k', alpha =0.5, marker='x')\n",
    "\n",
    "    cbar = plt.colorbar(shrink=0.5)\n",
    "    ci = plt.contour(x_spacing, y_spacing, smth_array.reshape(grid_shape), colors='black', linewidths=0.5)\n",
    "    ax.clabel(ci, inline=1, colors='black', fmt='%1.2f')\n",
    "    smth_plot_title = fm_nm + '_' + geoatt + '_' + str(rocol) + 'RC_' + str(error_thresh) + '_THRESHOLD_SMOOTH' + radius\n",
    "    smth_short_nm = fm_nm + '_' + geoatt + '_' + '_GRID_SMTH'\n",
    "    plt.title(smth_plot_title)\n",
    "\n",
    "    cnty_shapefile['coords'] = cnty_shapefile['geometry'].apply(lambda x: x.representative_point().coords[:])\n",
    "    cnty_shapefile['coords'] = [coords[0] for coords in cnty_shapefile['coords']]\n",
    "    \n",
    "    for idx, row in cnty_shapefile.iterrows():\n",
    "        plt.annotate(s=row['COUNTY'], xy=row['coords'],\n",
    "                 horizontalalignment='center', c='dimgrey')\n",
    "    cnty_shapefile.plot(ax=ax, facecolor=\"none\", edgecolor='dimgrey')\n",
    "    flt_shapefile2.plot(ax=ax, facecolor=\"none\", edgecolor='sienna')\n",
    "    prj_aoi.plot(ax=ax, facecolor=\"none\", edgecolor='red', linewidth=3)\n",
    "    plt.xlim(red_data_x.min()-10000, red_data_x.max()+10000)\n",
    "    plt.ylim(red_data_y.min()-10000, red_data_y.max()+10000)\n",
    "    \n",
    "    plot_text = 'Error threshold: ' + str(error_thresh) + '    Cutoff percentile: ' + str(error_thresh_pct) + '    Smooth radius: ' + str(radius)\n",
    "    ax.set_xlabel(plot_text, fontsize=12)\n",
    "    \n",
    "    plot_nm = file_path + fm_nm + '_' + geoatt + '_BF_SMTH_GRID' + str(comm) +'.jpg'\n",
    "    plot_nm1 = str(plot_nm)\n",
    "    new_path = PureWindowsPath(plot_nm1)\n",
    "    plt.savefig(plot_nm,bbox_inches='tight')    \n",
    "    if smth_popup == 1:\n",
    "        webbrowser.open(plot_nm)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    smth_grid_folderpathname = file_path + smth_short_nm + str(comm)+'.csv'\n",
    "    smth_output_df.to_csv(smth_grid_folderpathname, index=False)\n",
    "    \n",
    "else:\n",
    "    print('No smoothing applied')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Sample Smooth grid to df\n",
    "\n",
    "if radius != '0':\n",
    "\n",
    "    #Prep smth grid nodes as input data points\n",
    "    smth_data_z = smth_output_df[[smth_nm]].to_numpy()\n",
    "    smth_data_x = smth_output_df[['X']].to_numpy()\n",
    "    smth_data_y = smth_output_df[['Y']].to_numpy()\n",
    "\n",
    "    smth_red_data_z = smth_data_z.reshape(len(smth_data_z),)\n",
    "    smth_red_data_x = smth_data_x.reshape(len(smth_data_z),)\n",
    "    smth_red_data_y = smth_data_y.reshape(len(smth_data_z),)\n",
    "\n",
    "    xy_smth = merge(smth_red_data_x, smth_red_data_y)\n",
    "\n",
    "    #Prep original data locations as grid nodes\n",
    "    #orig_data_x = grid_eval_all_df[['X']].to_numpy()\n",
    "    orig_data_x = df_choice[['X']].to_numpy()\n",
    "    orig_data_red_x = orig_data_x.reshape(len(orig_data_x),)\n",
    "   # orig_data_y = grid_eval_all_df[['Y']].to_numpy()\n",
    "    orig_data_y = df_choice[['Y']].to_numpy()\n",
    "    orig_data_red_y = orig_data_y.reshape(len(orig_data_y),)\n",
    "\n",
    "    xy_smth_rev = merge(orig_data_red_x, orig_data_red_y)\n",
    "\n",
    "    #Train IDW model on smooth grid node values and locations\n",
    "    smth_idw_tree = tree(xy_smth, smth_red_data_z)\n",
    "\n",
    "    #Apply trained IDW model to original data xy's\n",
    "    smth_z_idw = smth_idw_tree(xy_smth_rev)\n",
    "\n",
    "    #Append new column of sampled smooth values to original data df\n",
    "    #grid_eval_all_df[smth_nm] = smth_z_idw\n",
    "    df_choice[smth_nm] = smth_z_idw\n",
    "    print(df_choice)\n",
    "    \n",
    "else:\n",
    "    print('No smoothed output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot the companion cumulative histogram plot\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "n, bins, patches = ax.hist(grid_eval_df[smp_diff_nm], 100, range=\n",
    "                           (grid_eval_df[smp_diff_nm].min(),grid_eval_df[smp_diff_nm].max()), density=True, \n",
    "                           histtype='step', cumulative=True, label='Sampled Grid Values', orientation='horizontal')\n",
    "\n",
    "#plt.hlines(y=error_thresh, xmin=0, xmax=1, color = 'red', linestyle ='dashed', linewidth = 1)\n",
    "\n",
    "ax.grid(True)\n",
    "ax.legend(loc='upper left')\n",
    "hist_title = 'Cumulative Frequency Plot\\n' + plot_title\n",
    "ax.set_title(hist_title)\n",
    "ax.set_ylabel('Z Value Abs Difference')\n",
    "plt.ylim(grid_eval_df[smp_diff_nm].min(),grid_eval_df[smp_diff_nm].max())\n",
    "plt.xlim(0,1)\n",
    "\n",
    "plot_text = 'Percentile\\nSummary: ' + (str(float(error_thresh_pct)*100)) + '% (' + (str(float(error_thresh_pct)*len(grid_eval_df))) + ' of ' + str(len(grid_eval_df)) + ') data points honored within ' + str(error_thresh) + ' units.'\n",
    "ax.set_xlabel(plot_text, fontsize=12)\n",
    "\n",
    "plot_nm = file_path + fm_nm + '_' + geoatt + '_OUTLIER_PLOT' + str(comm) +'.jpg'\n",
    "plot_nm1 = str(plot_nm)\n",
    "new_path = PureWindowsPath(plot_nm1)\n",
    "plt.savefig(plot_nm,bbox_inches='tight')    \n",
    "if cfp_popup == 1:\n",
    "    webbrowser.open(plot_nm)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('\\nUser-chosen acceptable error threshold: ', error_thresh)\n",
    "print('Error actual: ', error_act)\n",
    "print('User-chosen percentile outlier threshold: ', error_thresh_pct)\n",
    "print('Final number of rows and columns: ', rocol)\n",
    "print('\\n', grid_eval_df.describe(percentiles= [.05, .1, .25, .5, .75,.9, .95]))\n",
    "\n",
    "pts_folderpathname = file_path + short_nm + str(comm)+'_OR_PTS_EVAL.csv'\n",
    "pts_folderpathname_all = file_path + short_nm + str(comm)+'_ALL_PTS_EVAL.csv'\n",
    "grid_eval_df.to_csv(pts_folderpathname, index=False)\n",
    "#grid_eval_all_df.to_csv(pts_folderpathname_all, index=False)\n",
    "df_choice.to_csv(pts_folderpathname_all, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
